# -*- coding: utf-8 -*-
"""Boston_housing_important_dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rB2Udg47KGagt8gtH3NwotbEDCfx8242
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('/content/BostonHousing.csv')
df.head()

df.info()

"""## data visualization"""

df.hist(figsize=(5,5))
plt.show()

df.isnull().sum()

df['rm'].values

df['rm']=df['rm'].fillna(df['rm'].mean())

df.isnull().sum()

fig=plt.figure(figsize=(5,5))
for i in df.columns:
  sns.distplot(df[i])
  plt.show()

cols=['b','chas','zn','crim']
for i in cols:
  sns.boxplot(df[i])
  plt.show()

cols=['b','chas','zn','crim']
for i in cols:
  df[i]=np.log(df[i]+1)
  plt.show()

cols=['b','chas','zn','crim']
for i in cols:
  sns.distplot(df[i])
  plt.show()

cols=['b','chas','zn','crim']
for i in cols:
  sns.boxplot(df[i])
  plt.xlabel(i)
  plt.show()

import pandas as pd

# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = df['b'].quantile(0.25)
Q3 = df['b'].quantile(0.75)

# Calculate the IQR
IQR = Q3 - Q1

# Define the bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter the DataFrame to remove outliers
df_no_outliers = df[(df['b'] >= lower_bound) & (df['b'] <= upper_bound)]

print(df_no_outliers)

for i in df_no_outliers:
  sns.boxplot(df_no_outliers[i])
  plt.show()

df.info()

# therefore it is our new dataset
df_no_outliers.info()

for i in df_no_outliers:
  sns.distplot(df_no_outliers[i])
  plt.show()

df_no_outliers.isnull().sum()

data=df_no_outliers.corr()
figu=plt.figure(figsize=(10,10))
sns.heatmap(data,annot=True,cmap='coolwarm')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

x=df_no_outliers.drop('medv',axis=1)
y=df_no_outliers['medv']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
lr=LinearRegression()
lr.fit(x_train,y_train)
y_pred=lr.predict(x_test)
print('the accuracy is',lr.score(x_test,y_test))
from sklearn.model_selection import cross_val_score
scores=cross_val_score(lr,x,y,cv=5)
cv_score=np.mean(scores)
print('the cross val score is',scores.mean())
mse=mean_squared_error(y_test,y_pred)
print('the mean squared error is',mse)
print('the CV score is:',cv_score)

print('the intercepts are:',lr.intercept_)
print('the coeffients are:',lr.coef_)

sns.regplot(x=df_no_outliers['medv'],y=df_no_outliers['rm'])
plt.show()

sns.regplot(x=df_no_outliers['medv'],y=df_no_outliers['crim'])
plt.show()

from sklearn.tree import DecisionTreeRegressor
model=DecisionTreeRegressor()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
print('the accuracy is',model.score(x_test,y_test))

coef=pd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)
coef.plot(kind='bar',title='feature importance')

from sklearn.ensemble import RandomForestRegressor
model=RandomForestRegressor()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
print('the accuracy is',model.score(x_test,y_test))
coef=pd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)
coef.plot(kind='bar',title='feature importance')
#

from sklearn.ensemble import GradientBoostingRegressor
model=GradientBoostingRegressor()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
print('the accuracy is',model.score(x_test,y_test))
coef=pd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)
coef.plot(kind='bar',title='feature importance')
#

from xgboost import XGBRegressor
model=XGBRegressor()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
print('the accuracy is',model.score(x_test,y_test))
coef=pd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)
coef.plot(kind='bar',title='feature importance')
#

from sklearn.svm import SVR
model=SVR()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
print('the accuracy is',model.score(x_test,y_test))

